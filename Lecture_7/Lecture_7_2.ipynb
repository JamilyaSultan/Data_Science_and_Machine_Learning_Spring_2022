{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 7.2 Scikit-learn\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_7/Lecture_7_2.ipynb)\n",
    "\n",
    "\n",
    "[**Scikit-learn**](https://scikit-learn.org/stable/) is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. Key concepts and features include:\n",
    "\n",
    "* Algorithmic decision-making methods, including:\n",
    "    - Classification: identifying and categorizing data based on patterns.\n",
    "    - Regression: predicting or projecting data values based on the average mean of existing and planned data.\n",
    "    - Clustering: automatic grouping of similar data into datasets.\n",
    "\n",
    "\n",
    "* Algorithms that support predictive analysis ranging from simple linear regression to neural network pattern recognition.\n",
    "\n",
    "\n",
    "* Interoperability with NumPy, pandas, and matplotlib libraries.\n",
    "\n",
    "\n",
    "## Why Use Scikit-Learn For Machine Learning?\n",
    "\n",
    "Whether you are just looking for an introduction to ML, want to get up and running fast, or are looking for the latest ML research tool, you will find that scikit-learn is both well-documented and easy to learn/use. As a high-level library, it lets you define a predictive data model in just a few lines of code, and then use that model to fit your data. It’s versatile and integrates well with other Python libraries, such as matplotlib for plotting, numpy for array vectorization, and pandas for dataframes.\n",
    "\n",
    "Though scikit-learn has deep learning capabilities, one will typically use **tensorflow** or **pytorch** when working with deep learning models. However, most any other model that you work with will be best implemented with scikit-learn. \n",
    "\n",
    "In this notebook we introduce the basic concepts of the scikit-learn API, and in doing so, implement several of the algorithms that we have covered thus far in our course. First, we implement the $k$-nearest neighbors algorithm with the ```sklearn.neighbors.KNeighborsClassifier()``` Python class [[documentation]](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set Theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Load iris data \n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Grab versicolor and virginica flowers\n",
    "iris = iris[iris[\"species\"] != \"setosa\"]\n",
    "\n",
    "# Create feature matrix\n",
    "X = iris[[\"sepal_length\",\"sepal_width\"]].to_numpy()\n",
    "\n",
    "# Define labeling function\n",
    "def make_labels(y):\n",
    "    if y == \"versicolor\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "        \n",
    "# Create target value array\n",
    "y = iris[\"species\"].map(make_labels).to_numpy()\n",
    "\n",
    "# Create a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1)\n",
    "\n",
    "# Instantiate a KNN classifier \n",
    "clf = KNeighborsClassifier(n_neighbors = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be **fitted** to some data using its ```fit``` method.\n",
    "\n",
    "The fit method generally accepts 2 inputs:\n",
    "\n",
    "The samples matrix (or design matrix) ```X```. The size of ```X``` is typically ```(n_samples, n_features)```, which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "The target values ```y``` which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, ```y``` does not need to be specified. ```y``` is usually 1d array where the i-th entry corresponds to the target of the i-th sample (row) of ```X```.\n",
    "\n",
    "Both ```X``` and ```y``` are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Once the estimator is fitted, it can be used for predicting target values of new data. You don’t need to re-train the estimator\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"clf.predict(X_test) = {clf.predict(X_test)} \\n\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"y_pred == y_test = {y_pred == y_test} \\n\")\n",
    "\n",
    "print(f\"sum(y_pred == y_test)/len(y_test) = {sum(y_pred == y_test)/len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we can easily evaluate the accuracy of of our trained model by calling the ```score()``` method which returns the mean accuracy on the given test data and labels. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we can visualize the decision boundary generated by our trained classifier using the code below.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = clf)\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.title(\"Decision Boundaries\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Confusion Matrix \n",
    "A **confusion matrix** is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "\n",
    "> The confusion matrix shows the ways in which your classification model is confused when it makes predictions.\n",
    "\n",
    "It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made. It is this breakdown that overcomes the limitation of using classification accuracy alone.\n",
    "\n",
    "With the code below we visualize the confusion matrix for the $k$-nearest neighbors model on our current instance of train-test split. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Fit the model\n",
    "clf = KNeighborsClassifier(n_neighbors = 7)\n",
    "clf.fit(X_train,y_train)#Generate predictions with the model using our X values\n",
    "y_pred = clf.predict(X_test)#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"cf_matrix = {cf_matrix}\\n \")\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', cbar=False)\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Binary Classification Problems Are Special\n",
    "In a binary classification problem, we are often looking to discriminate between observations with a specific outcome, from normal observations.\n",
    "\n",
    "Such as a disease state or event from no disease state or no event.\n",
    "\n",
    "In this way, we can assign the event row as “positive” (label 1) and the no-event row as “negative“ (label 0). We can then assign the event column of predictions as “true” and the no-event as “false“.\n",
    "\n",
    "This gives us:\n",
    "\n",
    "* “true positive” for correctly predicted event values.\n",
    "* “false positive” for incorrectly predicted event values.\n",
    "* “true negative” for correctly predicted no-event values.\n",
    "* “false negative” for incorrectly predicted no-event values.\n",
    "\n",
    "**Precision** ($P$) is defined as the number of true positives ($T_p$) over the number of true positives plut the number of false positives ($F_p$):\n",
    "$$\n",
    "P = \\frac{T_p}{T_p + F_p}\n",
    "$$\n",
    "\n",
    "**Recall** ($R$) is defined as the number of true positives ($T_P$) over the number of true positives plut the number of false negatives ($F_n$):\n",
    "$$\n",
    "R = \\frac{T_p}{T_p + F_n}\n",
    "$$\n",
    "\n",
    "There quantities are related to the **$F_1$-score**, which is defined as the harmonic mean of precision and recall:\n",
    "$$\n",
    "F_1 = 2 \\frac{P\\times R}{P+R}\n",
    "$$\n",
    "\n",
    "Note that the precision may not decrease with recall. The definition of precision shows that lowering the threshold of a classifier may increase the denominator, by increasing the number of results returned. If the threshold was previously set too high, the new results may all be true positives, which will increase precision. If the previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing precision.\n",
    "\n",
    "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n",
    "\n",
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n",
    "\n",
    "A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
