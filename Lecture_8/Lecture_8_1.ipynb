{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 8.1 Decision and Regression Trees\n",
    "\n",
    "**Decision Trees (DTs)** are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Decision trees tend to be the method of choice for predictive modeling because they are relatively easy to understand and are also very effective. The basic goal of a decision tree is to split a population of data into smaller segments. There are two stages to prediction. The first stage is training the model—this is where the tree is built, tested, and optimized by using an existing collection of data. In the second stage, you actually use the model to predict an unknown outcome.\n",
    "\n",
    "Decision trees are constructed from only two elements — nodes and branches.\n",
    "\n",
    "<img src=\"Decision_Tree_Example1.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "This image was taken directly from this [article.](https://betterdatascience.com/mml-decision-trees/)\n",
    "\n",
    "\n",
    "The nodes shown above fall under the following types of nodes:\n",
    "\n",
    "* Root node — node at the top of the tree. This node acts as the input node for feature vectors in the model. \n",
    "* Decision nodes — nodes where the variables are evaluated. These nodes have arrows pointing to them and away from them\n",
    "* Leaf nodes — final nodes at which the prediction is made\n",
    "\n",
    "To illustrate how decision trees work we will consider artificial binary classification data generated by the ```sklearn.datasets.make_moons()``` function. One instance of this data is generated by running the following code cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "\n",
    "# The artificial data will be taken from sklearn (make_moons)\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 1_000, noise = 0.20, random_state=3)\n",
    "colors = [\"red\" if label == 0 else \"blue\" for label in y]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c = colors)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As with all machine learning tasks, we need split our data into a training and testing subsets. This can be done by calling the ```sklearn.model_selection.train_test_split()``` function. Run the following code cell to create this split and to visualize the training data. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "c_train = [\"red\" if label == 0 else \"blue\" for label in y_train]\n",
    "c_test = [\"red\" if label == 0 else \"blue\" for label in y_test]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = c_train)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we instantiate an instance of the ```sklearn.tree.DecisionTreeClassifier``` model. For documentation on this class see think [link](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier). Next we train the model by calling the ```fit()``` method. This can be shown by running the following code cell.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that our model has been trained we can visualize the tree structure of our current model by calling the ```sklearn.tree.plot_tree()``` function. An example of this can be seen by running the following code cell. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "tree_rules = export_text(decision_tree,\n",
    "                         feature_names = [\"x_0\", \"x_1\"])\n",
    "print(tree_rules, \"\\n\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "a = plot_tree(decision_tree,\n",
    "                   feature_names = [\"x_0\", \"x_1\"],\n",
    "                   class_names = [\"red\", \"blue\"],\n",
    "                   rounded = True,\n",
    "                   filled = True, \n",
    "                   fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The figure above depicts a graph theoretic tree that is used to make predictions. Suppose you would like to make a prediction on a given feature vector $x = [x_0, x_1]^T$. To do this, follow these steps: \n",
    "\n",
    "1. Start at the *root node* (depth 0, at the top).\n",
    "2. If $x_1 \\le 0.268$, then you then move down to the root's left child node (depth 1, left), otherwise move down to the root's right child node (depth 1, right). \n",
    "3. Repeat the process (illustrated in 2) of moving to successive child nodes according to satifying the boolean condition specified at each parent node until you reach a leaf node (a node with no child nodes). \n",
    "4. The predicted class of this leaf node will be the predicted class of our feature vector $x = [x_0, x_1]^T$.\n",
    "\n",
    "That's it! We can visualize the decision regions generated by our trained decision tree by running the following code cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = decision_tree)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As with all machine learning models, we next evaluate our models performance on the testing data. This is done by running the following two code cells. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values on the testing data\n",
    "test_pred_decision_tree = decision_tree.predict(X_test)\n",
    "\n",
    "# Import metrics from sklearn \n",
    "from sklearn import metrics\n",
    "\n",
    "# Note: visualizing your tree above will be weird after running seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# The confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, test_pred_decision_tree)\n",
    "\n",
    "# Convert confusion matrix into dataframe\n",
    "matrix_df = pd.DataFrame(confusion_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes()\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "sns.heatmap(matrix_df,\n",
    "            annot = True, \n",
    "            fmt = \"g\",\n",
    "            ax = ax, \n",
    "            cmap = \"magma\", \n",
    "            cbar = False)\n",
    "\n",
    "ax.set_title(\"Confusion Matrix - Decision Tree\")\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=15)\n",
    "ax.set_xticklabels([\"red\", \"blue\"])\n",
    "ax.set_ylabel(\"True Label\", fontsize=15)\n",
    "ax.set_yticklabels([\"red\", \"blue\"], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regression Trees\n",
    "In the following cell we load the [california housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). After which we attempt to perform regression on one of the columns of data using a regression tree. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
