{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 5.1 Single Neuron Logistic Regression\n",
    "\n",
    "In this notebook we revisit machine learning classification problems. More specifically, we consider the *probabilistic binary classfication problem*. We model this problem as a single neuron model with the *binary cross entropy loss function*. As with single neuron linear regression, we will train our neuron with stochastic gradient descent. \n",
    "\n",
    "## Why Probabilistic Modeling?\n",
    "In previous binary classification problems, such as when we applied the perceptron single neuron model, we were assuming that our data was linearly seperable. For example, consider the two figures generated by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import a nice function for plotting decision boudaries\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Set the Seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Read the iris dataset into a pandas DataFrame object with seaborn\n",
    "df = sns.load_dataset(\"iris\")\n",
    "\n",
    "setosa = df[df.species == \"setosa\"]\n",
    "versicolor = df[df.species == \"versicolor\"]\n",
    "virginica = df[df.species == \"virginica\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "\n",
    "ax1.scatter(setosa.sepal_length, \n",
    "            setosa.sepal_width, \n",
    "            color = \"red\", \n",
    "            label = \"setosa\")\n",
    "ax1.scatter(versicolor.sepal_length, \n",
    "            versicolor.sepal_width, \n",
    "            color = \"magenta\", \n",
    "            label = \"versicolor\")\n",
    "ax1.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax1.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax1.legend(fontsize = 15, loc = \"upper left\")\n",
    "ax1.set_title(\"Linearly Seperable Data\", fontsize = 18)\n",
    "\n",
    "\n",
    "ax2.scatter(versicolor.petal_length, \n",
    "            versicolor.sepal_length, \n",
    "            color = \"magenta\", \n",
    "            label = \"versicolor\")\n",
    "ax2.scatter(virginica.petal_length, \n",
    "            virginica.sepal_length, \n",
    "            color = \"lightseagreen\", \n",
    "            label = \"virginica\")\n",
    "ax2.set_xlabel(\"petal length [cm]\", fontsize = 15)\n",
    "ax2.set_ylabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax2.legend(fontsize = 15, loc = \"upper left\")\n",
    "ax2.set_title(\"Non-Linearly Seperable Data\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As you can see in the left figure generated above, it is impossible to split the versicolor and virginica flowers by any linear function. The reason for this is that the two species of flowers on the left overlap in the same regions of space. However, notice that in the non-linearly seperable case, the probability of selecting a versicolor flower is higher than the probability of selecting a virginica flower whenever petal length is less that 4.75. Conversely, the probability of selecting a virginica flower is higher than the probability of selecting a versicolor flower whenever the petal length is higher than 4.75. \n",
    "\n",
    "Observing this, label each versicolor flower by 0 and each virginica flower by 1. Then, only using the petal length as our feature measurment observe the figure generated by running the following code in the cell below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.xlim((2.75, 7 ))\n",
    "plt.scatter(versicolor.petal_length, \n",
    "            np.zeros(50), \n",
    "            color = \"magenta\", \n",
    "            label = \"versicolor\")\n",
    "plt.scatter(virginica.petal_length, \n",
    "            np.ones(50), \n",
    "            color = \"lightseagreen\", \n",
    "            label = \"virginica\")\n",
    "plt.vlines(4.75, 0.0, 1.0)\n",
    "plt.xlabel(\"petal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"label\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper left\")\n",
    "plt.title(\"Non-Linearly Seperable Data\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Designing a Single Neuron to Predict Probabilities\n",
    "Instead of creating a single neuron model for predicting a class deterministic label, we will next build a single neuron model that predicts a *class probability*. First, recall the general single neuron model depicted in the following figure. \n",
    "\n",
    "The Single Neuron Model\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"general_single_neuron.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### The Sigmoid Activation Function\n",
    "\n",
    "As before (both with the linear regression and Perceptron single neurons), we must first decide on an activation function before deciding on a cost/ loss function. For this purpose, we choose the *sigmoid* activation function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This differentiable function has a range in $(0, 1)$, so it would seem suitable for a possible function to turn the pre-activation value into a value representing a probability. Moreover, the sigmoid function (sometimes called the *logistic function*) has a smooth \"S\"-shape that is perfect for probabilities values transitioning, either growing or shrinking, as the input feature changes. For example, run the following code in the cell below. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "\n",
    "ax1.set_xlim((2.75, 7))\n",
    "ax1.scatter(versicolor.petal_length, \n",
    "            np.zeros(50), \n",
    "            color = \"magenta\", \n",
    "            label = \"versicolor\")\n",
    "ax1.scatter(virginica.petal_length, \n",
    "            np.ones(50), \n",
    "            color = \"lightseagreen\", \n",
    "            label = \"virginica\")\n",
    "ax1.set_xlabel(\"petal length [cm]\", fontsize = 15)\n",
    "ax1.set_ylabel(\"label\", fontsize = 15)\n",
    "ax1.set_title(\"Non-Linearly Seperable Data\", fontsize = 18)\n",
    "\n",
    "\n",
    "domain = np.linspace(-12.0, 12.0, 100)\n",
    "ax2.plot(domain, \n",
    "        sigmoid(domain), \n",
    "        color = \"blue\", \n",
    "        label = \"$\\sigma$(z)\")\n",
    "ax2.set_xlabel(\"z = wx + b\", fontsize = 18)\n",
    "ax2.set_ylabel(\"$sigma(z)$\", fontsize = 15)\n",
    "ax2.set_title(\"The Sigmoid Function\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As can be seen by the two figures generated above, the petal pre-activation value might be able to map the petal length measurements to the correct inputs to the sigmoid function so that the post-activation values correctly describe the probability of observing a versicolor flower or a virginica flower. In order to test this hypothesis, we must next introduce a cost/loss function to our single neuron model. \n",
    "\n",
    "## The Binary Cross Entropy Loss Function \n",
    "Currently we have two target values, 0 for versicolor and 1 for virginica. Moreover, we are wishing to predict that *probability of each of these labels given a single feature measurement*. Thus, we encounter the conditional probability function:\n",
    "\n",
    "$$\n",
    "P\\Big(y^{(i)}\\mid x^{(i)}\\Big)=\\begin{cases}\n",
    "          \\hat{y}^{(i)}, \\quad &  y^{(i)} = 1 \\\\\n",
    "          1-\\hat{y}^{(i)}, \\quad & y^{(i)} = 0 \\\\\n",
    "     \\end{cases}\n",
    "$$\n",
    "\n",
    "Notice that this conditional probability depends on the value of $\\hat{y}^{(i)}$, which in-turn depends on the values of our weight and bias. Moreover, we wish to *maximize* this probability over all training examples since this quantity is largest when our predicted probabilities are close approximations to the true 0-1 labels. Thus, we seek to solve the following maximization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}, b} \\sum_{i=1}^{N}P\\Big(y^{(i)}\\mid x^{(i)}\\Big).\n",
    "$$\n",
    "\n",
    "Before considering this optimization problem, we next recall the famous Bernoulli formula for binary probabilities:\n",
    "$$\n",
    "P\\Big(y^{(i)}\\mid x^{(i)}\\Big) = [\\hat{y}^{(i)}]^{y}[1 - \\hat{y}^{(i)}]^{(1-y)}\n",
    "$$\n",
    "\n",
    "Taking the logorithm on both sides of this equation yields (dropping the index notation to avoid messy equations):\n",
    "$$\n",
    "\\begin{align} \n",
    "\\log P\\Big(y^{(i)}\\mid x^{(i)}\\Big)&= \\log \\hat{y}^{y}(1 - \\hat{y})^{(1-y)}\\\\ \n",
    "&= y\\log \\hat{y} + (1-y) \\log (1 - \\hat{y})\\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the logorithmic function is an *increasing function*, maximimizing $P\\Big(y^{(i)}\\mid x^{(i)}\\Big)$ is equivalent to maximizing $\\log P\\Big(y^{(i)}\\mid x^{(i)}\\Big)$. Equivalently, we could also considering minimizing this function. Thus, we arrive at our single neuron coss/loss function for a single entry of data, which implies a full loss function. \n",
    "\n",
    "### Binary Cross Entropy Loss Function:\n",
    "$$\n",
    "L(\\mathbf{w}, b) = -\\frac{1}{N} \\sum_{i=1}^{N} P\\Big(y^{(i)}\\mid x^{(i)}\\Big) = \\frac{1}{N}\\sum_{i=1}^{N}\\Big[ -y^{(i)}\\log \\hat{y}^{(i)} - (1-y^{(i)}) \\log (1 - \\hat{y}^{(i)})\\Big ]\n",
    "$$\n",
    "\n",
    "Now that we have a plausible loss function, we have a complete single neuron model ready for training; see the figure below. \n",
    "\n",
    "The Logistic Regression Single Neuron Model\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"logistic_neuron.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Calculuting the Gradient of Binary Cross Entropy Loss Function\n",
    "In order to optimize the logistic regression single neuron model with stochastic gradient descent, we first need understand how to calculate the gradient. As before, we will consider the cost function on a single instance of data:\n",
    "\n",
    "$$\n",
    "C(w_1, b; x^{(i)},y^{(i)}) = -y^{(i)}\\log \\hat{y}^{(i)} - (1-y^{(i)}) \\log (1 - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "When considering this equation it is important to remember that $\\hat{y}^{(i)}$ really is a composite function. More specifically, we note\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(z) = \\sigma(w_1x^{(i)} + b).\n",
    "$$\n",
    "\n",
    "Next we note the particularly nice closed form of the derivative of the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "With these two equations, we are now ready to compute the partial derivatives of $C(w_1, b; x_{1}^{(i)},y^{(i)})$ with respect to $w_1$ and $b$. Note that this cost function contains two pieces, namely $-y^{(i)}\\log \\hat{y}^{(i)}$ and $- (1-y^{(i)}) \\log (1 - \\hat{y}^{(i)})$. Since the derivative is a linear map, we may calculate $\\partial C/ \\partial w_1$ by calculating the the derivative of each piece of this equation and then add them together. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial w_1}[-y^{(i)}\\log \\hat{y}^{(i)}] & = \\frac{\\partial}{\\partial w_1}[-y^{(i)}\\log \\sigma(w_1 x^{(i)}+b)] \\\\\n",
    " & = - \\frac{y^{(i)}}{\\sigma(w_1 x^{(i)}+b)}\\frac{\\partial}{\\partial w_1} [\\sigma(w_1 x^{(i)}+b)] \\\\\n",
    " & = - \\frac{y^{(i)}}{\\sigma(w_1 x^{(i)}+b)}\\sigma(w_1 x^{(i)}+b)(1 - \\sigma(w_1 x^{(i)}+b))\\frac{\\partial}{\\partial w_1}[w_1 x^{(i)}+b] \\\\\n",
    " & = - y^{(i)}(1 - \\sigma(w_1 x^{(i)}+b))x^{(i)} \\\\\n",
    " & = - y^{(i)}(1 - \\hat{y}^{(i)})x^{(i)} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial w_1}[-(1-y^{(i)}) \\log (1 - \\hat{y}^{(i)})] & = \\frac{\\partial}{\\partial w_1}[-(1-y^{(i)})\\log (1 - \\sigma(w_1 x^{(i)}+b))] \\\\\n",
    " & = - \\frac{(1 - y^{(i)})}{(1 - \\sigma(w_1 x^{(i)}+b))}\\frac{\\partial}{\\partial w_1} [1 - \\sigma(w_1 x^{(i)}+b) ]\\\\\n",
    " & = -  \\frac{(1 - y^{(i)})}{(1 - \\sigma(w_1 x^{(i)}+b))} -\\sigma(w_1 x^{(i)}+b)(1 - \\sigma(w_1 x^{(i)}+b))\\frac{\\partial}{\\partial w_1}[w_1 x^{(i)}+b] \\\\\n",
    " & = (1 - y^{(i)})\\sigma(w_1 x^{(i)}+b))x^{(i)} \\\\\n",
    " & = (1 - y^{(i)})\\hat{y}^{(i)}x^{(i)} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Now that we have calculated the derivative with respect to $w_1$ for each part of the binary cross entropy loss function, we next sum these derivatives:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C(w_1, b; x^{(i)},y^{(i)})}{\\partial w_1} & = - y^{(i)}(1 - \\hat{y}^{(i)})x^{(i)} + (1 - y^{(i)})\\hat{y}^{(i)}x^{(i)} \\\\\n",
    " & = [- y^{(i)}(1 - \\hat{y}^{(i)}) + (1 - y^{(i)})\\hat{y}^{(i)}]x^{(i)} \\\\\n",
    " & = [- y^{(i)} + y^{(i)}\\hat{y}^{(i)} + \\hat{y}^{(i)} - y^{(i)}\\hat{y}^{(i)}]x^{(i)} \\\\\n",
    " & = (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}\n",
    "\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "A similar calculation also yields the partial derivative of our cost function with respect to the bias $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(w_1, b; x^{(i)},y^{(i)})}{\\partial b} = (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "Notice how these partial derivatives precisely match the partial derivatives of the linear regression single neuron and the approximate partial derivatives of the Perceptron single neuron! Thus, we can train our logistic regression neuron in the exact same way as our previous models by implementing stochastic gradient descent. We now define our custom single neuron class for this purpose. \n",
    "ach feature vector. \n",
    " \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single artificial neuron. \n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : function\n",
    "        The activation function applied to the preactivation linear combination.\n",
    "    \n",
    "    cost_function : function\n",
    "        The cost function used to measure model performance.\n",
    "\n",
    "    w_ : numpy.ndarray\n",
    "        The weights and bias of the single neuron. The last entry being the bias. \n",
    "        This attribute is created when the train method is called.\n",
    "\n",
    "    errors_: list\n",
    "        A list containing the mean sqaured error computed after each iteration \n",
    "        of stochastic gradient descent per epoch. \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train(self, X, y, alpha = 0.005, epochs = 50)\n",
    "        Iterates the stochastic gradient descent algorithm through each sample \n",
    "        a total of epochs number of times with learning rate alpha. The data \n",
    "        used consists of feature vectors X and associated labels y. \n",
    "\n",
    "    predict(self, X)\n",
    "        Uses the weights and bias, the feature vectors in X, and the \n",
    "        activation_function to make a y_hat prediction on each feature vector. \n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function, cost_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def train(self, X, y, alpha = 0.005, epochs = 50):\n",
    "   \n",
    "        self.w_ = np.random.rand(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                self.w_[:-1] -= alpha*(self.predict(xi) - target)*xi\n",
    "                self.w_[-1] -= alpha*(self.predict(xi) - target)\n",
    "                #errors += .5*((self.predict(xi) - target)**2)\n",
    "                errors += self.cost_function(self.predict(xi), target)\n",
    "            self.errors_.append(errors/N)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preactivation = np.dot(X, self.w_[:-1]) + self.w_[-1]\n",
    "        return self.activation_function(preactivation)\n",
    "\n",
    "\n",
    "    def plot_cost_function(self):\n",
    "        fig, axs = plt.subplots(figsize = (10, 8))\n",
    "        axs.plot(range(1, len(self.errors_) + 1), \n",
    "                self.errors_,\n",
    "                label = \"Cost function\")\n",
    "        axs.set_xlabel(\"epochs\", fontsize = 15)\n",
    "        axs.set_ylabel(\"Cost\", fontsize = 15)\n",
    "        axs.legend(fontsize = 15)\n",
    "        axs.set_title(\"Cost Calculated after Epoch During Training\", fontsize = 18)\n",
    "        return fig\n",
    "\n",
    "    def plot_decision_boundary(self, X, y, xstring=\"x\", ystring=\"y\"):\n",
    "        plt.figure(figsize = (10, 8))\n",
    "        plot_decision_regions(X, y, clf = self)\n",
    "        plt.title(\"Logistic Regression Neuron Decision Boundary\", fontsize = 18)\n",
    "        plt.xlabel(xstring, fontsize = 15)\n",
    "        plt.ylabel(ystring, fontsize = 15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have defined a custom ```SingleNeuron``` class we are one step closer to training our model on the data at hand. However, before instantiating an instance of the ```SingleNeuron``` class we must first write a function for the binary cross entropy loss. Afterwards we may then create an instantance of our ```SingleNeuron```. Once this is done we need to convert the versicolor and virginica petal length measurements to a ```numpy.ndarray``` and reshape it into a column vector representation (**recall that this is necessary whenever our feature measurements consist of a single measurement**). We then need to create a target $y$ ```numpy.ndarray``` which assigns the labels 0 and 1 to the versicolor and virginica species, respectively. Once we have our feature vector and target vector we can then pass these values into the ```SingleNeuron.train()``` method to train our logistic single neuron with stochastic gradient descent. All of this can be done by running the following code in the cell below.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    return - y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat)\n",
    "\n",
    "node = SingleNeuron(sigmoid, cross_entropy_loss)\n",
    "\n",
    "X = df.iloc[50:].petal_length.values\n",
    "X = X.reshape(-1, 1)\n",
    "y = np.where(df.iloc[50:].species == \"versicolor\", 0, 1)\n",
    "\n",
    "node.train(X, y, alpha = 0.01, epochs = 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have trained the variable ```node```, we can now visualize the binary cross entropy loss over each epoch by plotting the values in the ```SingleNeuron.errors_``` attribute. This can be done by running the following code in the cell below. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.plot_cost_function()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This looks good! Visualizing the cost function over the epochs during training helps us verify that are neuron is indeed learning over time. We can next visualize our hypthesis function, or prediction function (recall in all supervised machine learning we are learning a function $h$ which approximates the true target function $f$), by plotting the ```SingleNeuron.predict()``` method over the scattered data points in question. This can be done by running the following code in the cell below.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.xlim((2.75, 7 ))\n",
    "plt.scatter(versicolor.petal_length, \n",
    "            np.zeros(50), \n",
    "            color = \"magenta\", \n",
    "            label = \"versicolor\")\n",
    "plt.scatter(virginica.petal_length, \n",
    "            np.ones(50), \n",
    "            color = \"lightseagreen\", \n",
    "            label = \"virginica\")\n",
    "\n",
    "domain = np.linspace(2.75, 7, 100)\n",
    "plt.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "plt.xlabel(\"petal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"label\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper left\")\n",
    "plt.title(\"The Learned Logistic Curve\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "When using the trained weights and bias of our logistic single neuron to predict a class on a given measurement, we will need to convert the predicted probabilities to zeros and ones. This can be done by passing the ```numpy.ndarray``` returned by the ```SingleNeuron.predict()``` method into the ```numpy.rint()``` method. Moreover, we can use the resulting ```numpy.ndarray``` to compute the classification error over our training data given by the formula\n",
    "\n",
    "$$\n",
    "E_c = \\frac{1}{N}\\sum_{i=1}^{N}[\\hat{y}^{(i)} \\neq y^{(i)}],\n",
    "$$\n",
    "\n",
    "where $[\\hat{y}^{(i)} \\neq y^{(i)}] = 1$ whenever $\\hat{y}^{(i)} \\neq y^{(i)}$, and zero otherwise. Run the following code in the cell below to view this classification error. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error = (np.rint(node.predict(X)) != y).astype(int)\n",
    "print(f\"Classification Error = {sum(classification_error)/ len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is a great classification error on our training data! We can now view our decision boundary implied by the trained weights and bias by running the following code in the cell below.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.plot_decision_boundary(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Logistic Regression Single Neuron with Multiple Inputs\n",
    "Thus far we have only used a single feature measurement as input into our logistic regression single neuron model, but what happens when we use *multiple measurements*. For example, we could use petal length and petal width. With two inputs our model can be depicted by the figure below.\n",
    "\n",
    "The Logistic Single Neuron Model with Multiple Feature Inputs\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"multi_logistic_neuron.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "Let us next instantiate a ```SingleNeuron``` model with petal length and sepal length measurements as input, train this model with the same learning rate and number of epochs as the variable ```node```, and then compare the cost function over epochs between the two models. This can be done by running the following code in the cell below. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new single neuron.\n",
    "node_two = SingleNeuron(sigmoid, cross_entropy_loss)\n",
    "\n",
    "# Create a numpy.ndarray of petal length and sepal length values for\n",
    "# the versicolor and virginica flowers, respectively. \n",
    "X_two = df.iloc[50:][[\"petal_length\", \"sepal_length\"]].values\n",
    "\n",
    "# Train the new single neuron model on the new feature vectors.\n",
    "node_two.train(X_two, y, alpha = 0.01, epochs = 10_000)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         label = \"node Cross Entropy Loss\")\n",
    "plt.plot(range(1, len(node_two.errors_) + 1), \n",
    "         node_two.errors_,\n",
    "         label = \" node_two Cross Entropy Loss\")\n",
    "plt.xlabel(\"epochs\", fontsize = 15)\n",
    "plt.ylabel(\"Cross Entropy Loss\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"Model Cost Comparison During Training\", fontsize = 18)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As we can see by the figures generated above, using two features results in a lower cost function (in this particular instance) with the same learning rate and number of epochs. We encourage the reader to compare different combinations of features from the data and compare the loss function over time during training. Finally, let us visualize the decision boundary generated by the trained weights and bias of ```node_two``` by running the following code in the cell below. After running notice that even with the logistic regression single neuron, we are still learning a linearly seperating hyperplane.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_two.plot_decision_boundary(X_two, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
