{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 4.2 Gradient Descent for Training Single Neuron Regression Models\n",
    "\n",
    "In this notebook we implement the single neuron model and the gradient descent algorithm in order to solve the supervised learning problem of **linear regression problem**. For teaching purposes, this notebook will focus on single variable regression for a single species of flower in the readily available iris dataset.  \n",
    "\n",
    "## 4.2.1 Regression\n",
    "Recall that in **regression** machine learning we are given labeled data $\\mathcal{D} = \\{(\\mathbf{x}^1, y^1), \\dots, (\\mathbf{x}^N, y^N)\\}$, where each feature vector satisfies $\\mathbf{x}^{(i)} \\in \\mathbb{R}$ and each label satifies $y^{(i)} \\in \\mathbb{R}$, and we wish to use the feature vectors to predict the real valued label (or target). Also recall that for each $i = 1, \\dots, N$, the feature vectors satisfy $\\mathbf{x}^{(i)} \\in \\mathcal{X}$ and the labels $y^{(i)} \\in \\mathcal{Y}$, where $\\mathcal{X}$ is the space of all possible feature vectors and $\\mathcal{Y}$ is the space of all possible associated labels. This is different from the classification perceptron as the following figure suggests.\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Regression_VS_Classification.png\" width=\"700\" hight =\"800\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### - Linear Regression\n",
    "\n",
    "The notion of **linear regression** if a specific case of regression where we assume that *each target value from $\\mathcal{D}$ is approximated by a linear function of its associated feature vector*. For example, consider the sepal length as inputs and sepal width as targets for each setosa flower in the iris dataset shown in the plot generated by running the following code in the cell below. \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "df = pd.read_csv(\"Datasets/iris_dataset.csv\")\n",
    "df = df.iloc[:50][[\"sepal_length\", \"sepal_width\"]]\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(df.sepal_length, \n",
    "            df.sepal_width, \n",
    "            color = \"lightseagreen\",\n",
    "            label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "df = pd.read_csv(\"Datasets/iris_dataset.csv\")\n",
    "df = df.iloc[:50][[\"sepal_length\", \"sepal_width\"]]\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(df.sepal_length, \n",
    "            df.sepal_width, \n",
    "            color = \"lightseagreen\",\n",
    "            label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2.2 Linear Regression Single Neuron Model\n",
    "\n",
    "As discussed in a [previous lecture](https://www.youtube.com/watch?v=SmEKxsd_67w&t=1s) there exists some hypothetical target function $f$ which assigns the correct labels to every possible feature measurement and machine learning models hope to approximate this target function $f$; see the figure below. \n",
    "\n",
    "---\n",
    "\n",
    "General ML Model:\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"General_ML_Model.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "In our current case we are assumming the target function $f$ is a **linear function of the input features**. Thus, with our single neuron model, we *should* choose a *linear-activation* for our single neuron model activation function. Note that we will make use of the *mean-sqaured error* cost function shown below. \n",
    "\n",
    "The perceptron is a single neuron model with the *sign* activation function as depicted in the figure below.\n",
    "\n",
    "---\n",
    "\n",
    "Single Neuron Regression Model\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"regression_neuron.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "Before defining a custom ```SingleNeuron``` class, we first need discuss how to use gradient descent to minimize the neurons cost function. More specifically, suppose that we our calculating the cost function for a *single example*, i.e., $N = 1$. For this single example we observe that the cost function is defined by: \n",
    "\n",
    "$$\n",
    "C(w, b; \\mathbf{x}^{(i)}, y^{(i)}) = \\frac{1}{2}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)^2. \n",
    "$$\n",
    "\n",
    "In the case of a linear activation function, it is important to note that $\\hat{y}^{(i)}$ is a very simple function of both $w_1$ and $b$. More specifically, we observe:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = a = z = w_1x^{(i)} + b. \n",
    "$$\n",
    "\n",
    "Thus, we may rewrite our neuron cost function with a single observation shown by the following equation:\n",
    "\n",
    "$$\n",
    "C(w, b; \\mathbf{x}^{(i)}, y^{(i)}) = \\frac{1}{2}\\Big(w_1x^{(i)} + b - y^{(i)}\\Big)^2. \n",
    "$$\n",
    "\n",
    "With this equation, we can calculate $\\partial C/ \\partial w_1$ and $\\partial C/ \\partial b$ easily by applying the [chain rule (click for a quick refresher on the concept)](https://www.youtube.com/watch?v=HaHsqDjWMLU); the resulting partial derivatives with respect to $w_1$ and $b$ shown by the following equations:\n",
    "\n",
    "1. $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1} = (w_1x^{(i)} + b - y^{(i)})x^{(i)} = (\\hat{y}^{(i)} - y^{(i)})x^{(i)}$\n",
    "2. $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial b} = (w_1x^{(i)} + b - y^{(i)}) = (\\hat{y}^{(i)} - y^{(i)})$\n",
    "\n",
    "Understanding the different ways in which we may calculate the partial derivatives of our cost function is essential in applying any *first-order* minimization technique on the cost function $C(w_1, b)$. With what follows we discuss two of the three fundamental methods used to accomplish this goal. \n",
    "\n",
    "\n",
    "## 4.2.3 Minimize the Cost Function $C(w_1, b)$\n",
    "In a previous lecture we learned how the iterative technique called gradient descent can be used to find local minima of differentiable functions. Since our single neuron models cost function is a differentiable function of $w_1$ and $b$, **we can apply this technique to minimize the cost function**, which in-turn will make our neuron better at predicting the correct target label on the data we are observing. Referencing our general ML model depicted above we are *using gradient descent to search through the hypothesis space to generate a suitable hypothesis made by our model*. \n",
    "\n",
    "\n",
    "### - Different Flavors of First-Order Minimization \n",
    "In the previous single observation example, we easily calculated $\\frac{\\partial C}{\\partial w_1}$ and $\\frac{\\partial C}{\\partial b}$ by applying the chain-rule. With this observation on a single observation, we can easily extend this notion to all data used in training by summing the gradient calculated at entry of data. We will refer to this process as calculating the **full gradient** (or **full partial derivatives**) with respect to the training data: \n",
    "\n",
    "1. $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1} = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)x^{(i)}$\n",
    "2. $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b} = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)$\n",
    "\n",
    "**Flavor 1. Batch Gradient Descent Algorithm:**\n",
    "1. For each epoch **do**\n",
    "2. Calculate the full gradient by finding $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1}$ and $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b}$.\n",
    "3. $w \\leftarrow w - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1}$\n",
    "4. $b \\leftarrow b - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b}$\n",
    "\n",
    "Applying batch gradient descent will work *but can be very slow and use a lot of memory* when the number of feature vectors is large (possibly millions). More importantly, **batch gradient descent is not necessary to find local minima**. The most common way work around this problem is to update $w$ and $b$ by calculating the gradient with respect to one entry of data at a time. This technique is called **stochastic gradient descent** and is one of the primary tools in training deep neural networks and simple single neuron models.  \n",
    "\n",
    "**Flavor 2. Stochastic Gradient Descent Algorithm:**\n",
    "1. For each epoch **do**\n",
    "2. For $i = 1, \\dots, N$ **do**\n",
    "3. Calculate $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1}$ and $\\frac{\\partial C(\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)}))}{\\partial b}$.\n",
    "2. $w \\leftarrow w - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1}$\n",
    "3. $b \\leftarrow b - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial b}$\n",
    "\n",
    "For single neuron models in practice, stochastic gradient descent should be the preferred way for optimizing the weights and bias by minimizing the cost function. We implement stochastic gradient descent with the ```train``` method used in the following custom ```SingleNeuron``` class. \n",
    "```python\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single artificial neuron. \n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : function\n",
    "        The activation function applied to the input linear combination.\n",
    "        \n",
    "    w_ : numpy.ndarray\n",
    "        The weights and bias of the single neuron. The last entry being the bias. \n",
    "        This attribute is created when the train method is called.\n",
    "        \n",
    "    errors_: list\n",
    "        A list containing the mean sqaured error computed after each iteration \n",
    "        of stochastic gradient descent per epoch. \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train(self, X, y, alpha = 0.005, epochs = 50)\n",
    "        Iterates the stochastic gradient descent algorithm through each sample \n",
    "        a total of epochs number of times with learning rate alpha. The data \n",
    "        used consists of feature vectors X and associated labels y. \n",
    "        \n",
    "    predict(self, X)\n",
    "        Uses the weights and bias, the feature vectors in X, and the \n",
    "        activation_function to make a y_hat prediction on each feature vector. \n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def train(self, X, y, alpha = 0.005, epochs = 50):\n",
    "        self.w_ = np.random.rand(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                self.w_[:-1] -= alpha*(self.predict(xi) - target)*xi\n",
    "                self.w_[-1] -= alpha*(self.predict(xi) - target)\n",
    "                errors += .5*((self.predict(xi) - target)**2)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preactivation = np.dot(X, self.w_[:-1]) + self.w_[-1]\n",
    "        return self.activation_function(preactivation)\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Quick Data Formatting\n",
    "\n",
    "Now that we have defined our custom ```SingleNeuron``` class we next prep our data for training. By running the following code in the cell below the first two lines convert the setosa sepal length column of data to a numpy vector representation (single entry feature vectors need to be reshaped using the ```reshape(-1, 1)``` method to format the data as a column vector), and the third line converts the setosa sepal width column of data to a ```numpy.ndarray```. \n",
    "```python\n",
    "X = df.sepal_length.values\n",
    "X = X.reshape(-1, 1)\n",
    "y = df.sepal_width.values\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can double check that we have the appropriate data by scatter plotting the data again. Run the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\")\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Creating and Training an Instance of the ```SingleNeuron``` Class\n",
    "\n",
    "In order to instantiate a given instance of our ```SingleNeuron``` model, we need to first define an activation function. After doing so we can instantiate a ```SingleNeuron``` object as well as train it by calling the ```train()``` method with input ```X``` and ```y```. Do this by running the following code in the cell below. \n",
    "```python\n",
    "def linear_activation(z):\n",
    "    return z\n",
    "\n",
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = .0001, epochs = 5)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have created an instance of the ```SingleNeuron``` class and called the train method, we can visualize the linear regression line by scatter plotting the data and also ploting the predicted output over some domain within the range of values of input features. Do this by running the following code in the cell below. \n",
    "```python\n",
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\")\n",
    "plt.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\")\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The line generated by our custom ```SingleNeuron``` class surely does not look random! We can verify this by plotting the ```errors_``` attribute that we stored while training, which computed the *mean-sqaured error* on the training dataset after each epoch of applying the stochastic gradient descent algorithm with the ```train()``` method. If the mean sqaured error is decreasing after each epoch we are on the right track, and our single neuron might be learning! \n",
    "\n",
    "We can easily visualize the mean-sqaured error at each epoch of our training process by running the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "plt.xlabel(\"epochs\", fontsize = 15)\n",
    "plt.ylabel(\"MSE\", fontsize = 15)\n",
    "plt.xticks(range(1, len(node.errors_) + 1))\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Visualize your Errors over each Epoch\n",
    "\n",
    "Yes, the MSE is decreasing over each epoch! Lets next try training a single neuron over 10 times more epochs with the same learning rate and see what happens. Try running the following code in the cell below (note the use of the subplots). \n",
    "```python\n",
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = .0001, epochs = 50)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "\n",
    "ax1.scatter(X, y, color = \"lightseagreen\")\n",
    "ax1.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "ax1.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax1.set_ylabel(\"sepal width [cm]\")\n",
    "ax1.set_title(\"Setosa Regression Data\", fontsize = 18)\n",
    "\n",
    "ax2.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylabel(\"MSE\")\n",
    "ax2.set_xticks(range(1, len(node.errors_) + 1, 5))\n",
    "ax2.legend(fontsize = 10)\n",
    "ax2.set_title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2.3 Experiment with the Learning Rate \n",
    "As discussed in a previous notebook, the choice of learning rate is a crucial *hyperparamter* when implementing gradient (and stochastic gradient) descent. We can view different choices of learning rate with a fixed number of epochs by running the following code in the cell below.\n",
    "```python\n",
    "# Possible choices of learning rate \n",
    "alphas = [0.01, 0.05, 0.07, 0.09]\n",
    "\n",
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "# Call the subplots method for plotting a grid of figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# Loop over the axs and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    node = SingleNeuron(linear_activation)\n",
    "    node.train(X, y, alpha = alpha, epochs = 55)\n",
    "    ax.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "    ax.scatter(X, y, color = \"lightseagreen\")\n",
    "    ax.set_title(f\"alpha = {alpha}\", fontsize = 18)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
