{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 4.1. Gradient Descent\n",
    "\n",
    "As observed in a previous lecture, we may train a single neurons weights and bias by iteratively applying a predefined rule. The description that we gave for the perceptron update rule was intended to hint at a more general rule for *optimizing the cost function of a model*. More specifically, we were hinting at a general continuous optimization technique called **gradient descent**. \n",
    "\n",
    "In order to understand gradient descent we will need to import the following python packages and define a simple function of one variable. \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn theme for plots\n",
    "sns.set_theme()\n",
    "\n",
    "# Define test function for experimenting\n",
    "def f(w):\n",
    "    return (w - 2)**2  + 1\n",
    "\n",
    "# Define domain variables for plotting f\n",
    "domain = np.linspace(-2, 6, 50)\n",
    "\n",
    "# Plot the function f(w)\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"f(w) = (w - 2)^2 + 1\")\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(w)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.1 The Problem Description\n",
    "\n",
    "Clearly the function $f(w) = (w - 2)^2 + 1$ has a **global minimum** at $w = 2$. Supposing we did not already know the answer..., this notebook will describe how to find this minimum value for $f(w)$. More specifically, we wish to solve\n",
    "$$\n",
    "\\min_{w\\in \\mathbb{R}} f(w) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{PROBLEM 1})\n",
    "$$\n",
    "\n",
    "This class of problems falls into the reahlm of *unconstrained continuous optimization*. For those of you that are more mathematically inclined, see the excellent and classic text of [Nocedal and Wright](http://egrcc.github.io/docs/math/numerical-optimization.pdf) for an in-depth treatment of the subject.\n",
    "\n",
    "The treatment of PROBLEM 1 given by Nocedal and Wright is extensive and goes much further than what is currently implemented in machine learning. We will focus on the notion of using the *gradient* (the generalization of the single variable function derivative) in order to \"search\" for plausable minimum of a function. \n",
    "\n",
    "### Start with a Guess\n",
    "Suppose we first guess that the minimum value of $f(w)$ occurs at $w_0 = 5$. We can visualize the point $(5, f(5))$ by running the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "plt.scatter([5], [f(5)], color = \"magenta\", label = \"$w_0$: initial guess\")\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### The General Idea \n",
    "The general idea behind gradient descent is to use the gradient (the derivative for single variable functions) to *provide a direction to explore*. For example, with our function $f$ and initial guess $w_0 = 5$, suppose we are able to calculate the value of the *gradient* (the derivative) of $f$ at $w_0 = 5$. This numerical value will give us the *slope of the tangent line* to $f$ at $w_0$. Note that $f'(w) = 2(w - 2)$. We can even visualize this tangent line by running the following code in the cell below. \n",
    "```python\n",
    "# Define parabola derivative\n",
    "def df(w): \n",
    "    return 2*(w - 2)\n",
    "\n",
    "# Choose w_0\n",
    "w_0 = 5.0\n",
    "\n",
    "# Define tangent line function for visualization\n",
    "def tangent_line(w_i, function, derivative, i = 0, color = \"magenta\", show = True):\n",
    "    # Define tangent line\n",
    "    # y = m*(x - x1) + y1\n",
    "    def line(w):\n",
    "        return derivative(w_i)*(w - w_i) + function(w_i)\n",
    "    \n",
    "    wrange = np.linspace(w_i - 1, w_i + 1, 10)\n",
    "    if show:\n",
    "        plt.plot(wrange,\n",
    "            line(wrange), \n",
    "            'C1--', \n",
    "            linewidth = 1, \n",
    "            color = \"red\", \n",
    "            label = \"tangent line\")\n",
    "    plt.scatter([w_i], [function(w_i)], color = color, label = f\"w_{i}\")\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "# Plot the function\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "# Visualize the tangent line\n",
    "tangent_line(w_0, f, df)\n",
    "\n",
    "# Label the figure\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"$\\min_w$ $f(x)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.2 Direction of Descet and the Learning Rate\n",
    "\n",
    "Observing the figure generated by the code above, notice that the tangent line is pointing in the direction of descent, i.e., starting at $w_0 = 5$, the tangent line having positive slope indicates that we need move in the negative direction (to the left of $w_0$) if we wish to move to a smaller value of $f$. That is, **we need to move in the direction implied by the opposite sign of the derivtaive of $f$ at $w_0 = 5$**.\n",
    "\n",
    "### But how far should we move? \n",
    "The value of how far to move in the opposite sign of the gradient of $f$ at $w_0 = 5$ from $w_0$ is called the **learning rate** (Nocedal & Wright call this parameter the **step length**), typically denoted by $\\alpha$. In gradient descent this value is multiplied with the gradient of $f$ at $w_0$ (the derivative for one variable), and then a new choice of $w$, say $w_1$ is assigned the value:\n",
    "$$\n",
    "w_1 = w_0 - \\alpha f'(w_0) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{Gradient Descent Update Rule for a Function of one Variable})\n",
    "$$\n",
    "\n",
    "The choice of $\\alpha$ in machine learning is typically found by experimentation, though more sophesticated techniques are available, such as *line-search* and *trust-region* methods (again see Nocedal & Wright). \n",
    "\n",
    "For example, lets choose $\\alpha = 0.8$ and run the following code in the cell below. \n",
    "```python\n",
    "# Initialize choice of w\n",
    "w_0 = 5.0\n",
    "\n",
    "# Set learning rate \n",
    "alpha = .8\n",
    "\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_1 = w_0 - alpha*df(w_0)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notice that the new guess $w_1$ gives a new pair $(w_1, f(w_1))$ which is a better choice of a both a *extrema* and minimum value for $f$ than the initial guess $w_0$ would have given. **We are moving downhill on the function $f$.** Let us move again and see where we end up by running the following code in the cell below. \n",
    "```python\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_2 = w_1 - alpha*df(w_1)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\", show = False)\n",
    "tangent_line(w_2, f, df, i = 2, color = \"red\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "And one more time by running the following code in the cell below.\n",
    "```python\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_3 = w_2 - alpha*df(w_2)\n",
    "\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\", show = False)\n",
    "tangent_line(w_2, f, df, i = 2, color = \"red\", show = False)\n",
    "tangent_line(w_3, f, df, i = 3, color = \"purple\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can easily iterate this process of updating $w_i$ by writing a function called ```gradient_descent```. Try running the following code in the cell below. \n",
    "```python\n",
    "def gradient_descent(df, alpha = 0.8, w_0 = 5.0, max_iter = 1_000):\n",
    "    W = [w_0]\n",
    "    i = 0\n",
    "    while abs(df(W[-1])) > 0.001 and i < max_iter:\n",
    "        w_new = W[-1] - alpha*df(W[-1])\n",
    "        W.append(w_new)\n",
    "        i += 1\n",
    "    W = np.array(W)\n",
    "\n",
    "    return W\n",
    "\n",
    "W = gradient_descent(df)\n",
    "\n",
    "for i, w in enumerate(W):\n",
    "    print(f\"w_{i} = {np.round(w, decimals = 2)} | df(w_{i}) = {df(w)}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "plt.scatter(W, f(W), color = \"magenta\")\n",
    "tangent_line(W[-1], f, df, i = len(W), color = \"blue\")\n",
    "plt.xlabel(\"$w$ (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"$f(w)$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(f\"alpha = {alpha}, Iterations = {len(W)}\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing different choices of $\\alpha$\n",
    "We can test how well our choice of the learning rate $\\alpha$ was by comparing it to different choices of $\\alpha$. Try running the following code in the cell below. What do you observe?\n",
    "```python\n",
    "# Possible choices of learning rate \n",
    "alphas = [0.8, 0.1, .9, .5]\n",
    "\n",
    "# Call the subplots method for plotting a grid of figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# Loop over the axs and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    W = gradient_descent(df, alpha = alpha)\n",
    "    ax.plot(domain, f(domain))\n",
    "    ax.scatter(W, f(W), color = \"magenta\")\n",
    "    ax.set_title(f\"alpha = {alpha}, Iterations = {len(W)}\", fontsize = 18)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "One very important note to make with this example is that gradient descent will always converge to a *global optimal* solution for any *convex function*, such as the quadratic function we have been playing with. This is not always the case as can be seen with the following code, of which you should run in the cell below. \n",
    "```python\n",
    "def C(w):\n",
    "    return (w**2*np.cos(w) - w)/10\n",
    "\n",
    "def dC(w, h = 0.001):\n",
    "    return (C(w + h) - C(w))/h\n",
    "\n",
    "W = gradient_descent(dC, alpha = 0.8, w_0 = 10, max_iter=5_000)\n",
    "\n",
    "# New domain \n",
    "domain = np.linspace(-15, 15, 100)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, C(domain), label = \"$C(w) = (w^2\\cos(w) - w)/10$\")\n",
    "tangent_line(W[-1], C, dC, i = len(W))\n",
    "\n",
    "plt.xlabel(\"$w$ (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"$C(w)$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(f\"alpha = {alpha}, w_0 = {W[0]}, Iterations = {len(W)}\", fontsize = 18)\n",
    "plt.annotate(\"Local Minima\", (W[-1], C(W[-1])), horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gradient_descent(dC, alpha = 0.01, w_0 = 15, max_iter=5_000)\n",
    "\n",
    "# New domain \n",
    "domain = np.linspace(-25, 25, 200)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, C(domain), label = \"$C(w) = (w^2\\cos(w) - w)/10$\")\n",
    "tangent_line(W[-1], C, dC, i = len(W))\n",
    "\n",
    "plt.xlabel(\"$w$ (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"$C(w)$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(f\"alpha = {alpha}, w_0 = {W[0]}, Iterations = {len(W)}\", fontsize = 18)\n",
    "plt.annotate(\"Local Minima\", (W[-1], C(W[-1])), horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W = gradient_descent(dC, alpha = 0.01, w_0 = 20, max_iter=5_000)\n",
    "\n",
    "# New domain \n",
    "domain = np.linspace(-45, 45, 200)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, C(domain), label = \"$C(w) = (w^2\\cos(w) - w)/10$\")\n",
    "tangent_line(W[-1], C, dC, i = len(W))\n",
    "\n",
    "plt.xlabel(\"$w$ (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"$C(w)$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(f\"alpha = {alpha}, w_0 = {W[0]}, Iterations = {len(W)}\", fontsize = 18)\n",
    "plt.annotate(\"Local Minima\", (W[-1], C(W[-1])), horizontalalignment='right', verticalalignment='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
