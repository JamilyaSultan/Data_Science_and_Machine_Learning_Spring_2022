{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 9.1 Ensemble Methods\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_9/Lecture_9_1.ipynb)\n",
    "\n",
    "**Ensemble methods** are machine learning methods that aggregate the predictions of a group of base learners in order to form a single learning model. For example, imagine that each person in this [video](https://www.youtube.com/watch?v=iOucwX7Z1HU&t=203s) is a trained machine learning model and notice the average of their predictions is a much more accurate prediction that the individual predictions. In this lecture we will consider two types of ensemble concepts and methods. Namely, \n",
    "1. **Bagging**\n",
    "\n",
    "2. **Random Forests**\n",
    "\n",
    "In the rest of this notebook we explore these concepts. \n",
    "\n",
    "---\n",
    "\n",
    "## 9.1.1 Bagging\n",
    "\n",
    "<img src=\"Bootstrapping.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The term **bagging** referes to **b**ootstrap **agg**regating. **Bootstrapping** is a method of inferring results for a population from results found on a collection of smaller random samples of that population, using replacement during the sampling process. In the context of machine learning, a given set of machine learning model is trained respectively on random samples of training data with replacement (see the above figure), then the combined predictions of each model is **aggregated** and used as a single prediction. For regression tasks this would mean taking the average of the set of model prediction, and for classification taking the majority vote.  \n",
    "\n",
    "Generally speaking, the models we pick for ensembling will be \"dumb learners\", meaning models that are barely superior to randomly guessing. Individually the models will perform poorly, but collectively will perform well. \n",
    "\n",
    "But why is bagging useful? According to this [article](https://towardsdatascience.com/random-forests-algorithm-explained-with-a-real-life-example-and-some-python-code-affbfa5a942c):\n",
    "\n",
    "> \"Each model is trained on a different dataset, because theyâ€™re bootstrapped. So inevitably, each model will make different mistakes, and have a distinct error and variance. Both the error and variance get reduced in the aggregation step where, literally in the case of Regression, they are averaged out.\"\n",
    "\n",
    "The key idea here is the reduction in variance of the model. Again, from the above article:\n",
    "\n",
    "> \"In a Regression task you can calculate actual variance of the prediction compared to the true targets. If the tree produces results that are too far off from its true targets, it has high-variance and therefore, it is overfit.\"\n",
    "\n",
    "To illustrate this concept we will once again consider the iris dataset; and compare the performance between a single decision tree model and a bagging classifier of many depth 1 decision trees, referred to as *decision stumps*. Run the following code cell to load the iris dataset and visualize the data. \n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set theme for plotting\n",
    "sns.set_theme()\n",
    "\n",
    "# Import the data\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris = iris.iloc[50:]\n",
    "\n",
    "X = iris[[\"sepal_length\", \"sepal_width\"]].to_numpy()\n",
    "\n",
    "# Define labeling function\n",
    "def make_labels(y):\n",
    "    if y == \"versicolor\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "        \n",
    "# Create target value array\n",
    "y = iris[\"species\"].map(make_labels).to_numpy()\n",
    "\n",
    "# Plot the data\n",
    "flowers = [\"versicolor\", \"virginica\"]\n",
    "colors = [\"magenta\", \"lightseagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "for species, color in zip(flowers, colors):\n",
    "    temp_df = iris[iris.species == species]\n",
    "    ax.scatter(temp_df.sepal_length,\n",
    "               temp_df.sepal_width,\n",
    "               c = color,\n",
    "               label = species, \n",
    "               )\n",
    "    \n",
    "ax.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "As you can tell, no one model would ever hope to learn seperation on this dataset. Let us next compare a decision tree. \n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "                            n_estimators = 100,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = tree_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = bag_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Random Forests \n",
    "Technically speaking, the above bagging model is called a **Random forest**. Such a model exists inside the ```sklearn.ensemble``` module, and is the ```DecisionTreeClassifier``` class. However, the random forest algorithm used in training the ```DecisionTreeClassifier``` class introduces extra randomness when growing trees; instead of searching for the best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater diversity of trees which results in even lower variance of the fit model. \n",
    "\n",
    "Run the following code cell and compare the three models. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "                            n_estimators = 100,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")\n",
    "\n",
    "forest_clf = RandomForestClassifier(max_depth = 1, n_estimators = 100,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Forest Classification Report\")\n",
    "print(classification_report(y_test, forest_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As obserbed in the output of the above code cell, the accuracy of the bagging classifier with decision stumps and the random forest are the same. However, in theory the random forest model will have less variance than the random forest. Another way we can compare these two models with identical accuracy is to view the decision regions generated by the. Let's next do this by running the following three code cells. \n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = tree_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = bag_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plot_decision_regions(X, y, clf = forest_clf)\n",
    "plt.xlabel(\"feature x_0\", fontsize = 15)\n",
    "plt.ylabel(\"feature x_1\", fontsize = 15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "It may be the case that we can improve the performance of all three models by considering more features in the dataset. With this in mind, let's consider these three models on all 4 features of the dataset. This can be done by running the following code cell. \n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 2)\n",
    "\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=15, random_state=2,)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_y_pred = tree_clf.predict(X_test)\n",
    "print(f\"Tree Classification Report\")\n",
    "print(classification_report(y_test, tree_y_pred), \"\\n\")\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1, random_state=2),\n",
    "                            n_estimators = 100,\n",
    "                            bootstrap = True,\n",
    "                            n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Bagging Classification Report\")\n",
    "print(classification_report(y_test, bag_y_pred), \"\\n\")\n",
    "\n",
    "forest_clf = RandomForestClassifier(max_depth = 1, n_estimators = 100,\n",
    "                                    bootstrap = True,\n",
    "                                    n_jobs = -1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_y_pred = bag_clf.predict(X_test)\n",
    "print(f\"Forest Classification Report\")\n",
    "print(classification_report(y_test, forest_y_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance \n",
    "\n",
    "One cool thing about random forests is that these models make it simple to measure feature importance of each feature. Scikit-Learn does this by measuring a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average across all trees in the forest. For example, consider running the following code cell. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "for name, score in zip(names, forest_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
